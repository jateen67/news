{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8oXoONPY7_d",
        "outputId": "f304cc3d-0987-4166-85c7-e388befa106f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/news\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/news/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tO-iTbYogAH",
        "outputId": "5597ba6f-f0ee-42fc-9737-f028aba9cdb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import LSTM, Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3btXoai3reVj",
        "outputId": "307fb388-b854-4e2e-9c84-49bf3dc50b4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
        "# safety check\n",
        "imdb_reviews.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZLI7YQr-ma"
      },
      "source": [
        "preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mBx1dLossAEM"
      },
      "outputs": [],
      "source": [
        "def preprocess(s):\n",
        "    s = s.lower()\n",
        "\n",
        "    s = re.compile(r'<[^>]+>').sub('', s)\n",
        "\n",
        "    # remove punctuations/numbers\n",
        "    s = re.sub('[^a-zA-Z]', ' ', s)\n",
        "\n",
        "    # remove single letters\n",
        "    s = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', s)\n",
        "\n",
        "    # remove places with more than one space\n",
        "    s = re.sub(r'\\s+', ' ', s)\n",
        "\n",
        "    # remove nltk stopwords (regex makes it go wayyy faster than the line below)\n",
        "    # s = ' '.join([word for word in s.split() if word not in (stopwords.words('english'))])\n",
        "    s = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*').sub('', s)\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHIxejCcwkG6",
        "outputId": "c848460f-2b63-4393-9157-27bc30266ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use rust blllllazingly fast code amazing\n"
          ]
        }
      ],
      "source": [
        "print(preprocess(\"i use ` rust for BLLLLLAZINGLY fast code    ... its amazing\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0o38gkn1iLP",
        "outputId": "a02a3fe0-030e-4cee-c6e6-f9e6776846c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wonderful little production filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great master comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell murals decorating every surface terribly well done \n"
          ]
        }
      ],
      "source": [
        "# preprocessing all reviews\n",
        "preprocessed_text = []\n",
        "\n",
        "for review in list(imdb_reviews['review']):\n",
        "    preprocessed_text.append(preprocess(review))\n",
        "\n",
        "print(preprocessed_text[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXo0bfqi22W8",
        "outputId": "e93e59e7-d033-44ce-ec1f-0f0ef7da11b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# converting each review's \"sentiment\" column value to 1 and 0 instead of \"positive\" and \"negative\"\n",
        "sentiments = []\n",
        "\n",
        "for sentiment in imdb_reviews['sentiment']:\n",
        "    if sentiment == \"positive\":\n",
        "        sentiments.append(1)\n",
        "    else:\n",
        "        sentiments.append(0)\n",
        "\n",
        "sentiments = np.array(sentiments)\n",
        "\n",
        "print(sentiments[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OLjTfAOJ4T66"
      },
      "outputs": [],
      "source": [
        "# spliting data into training and test set data for the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_text, sentiments, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxPbK5dqTkzE"
      },
      "source": [
        "making embedded layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZAmBoj4RSfgG"
      },
      "outputs": [],
      "source": [
        "# transform raw text into numerical representations suitable for feeding into the model\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcNIN5fWV4Vh",
        "outputId": "9c18eab4-967e-427a-b305-8daf1a8bd4e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90095\n"
          ]
        }
      ],
      "source": [
        "# our corpus has 90,094 words in it\n",
        "# also adding one more to the mix for words that dont have a word embedding (90,095 total)\n",
        "corpus_length = len(tokenizer.word_index) + 1\n",
        "print(corpus_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PnAmlLbwW5-p"
      },
      "outputs": [],
      "source": [
        "# padding reviews to 100 characters\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=100)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "frkhAa_SXY00"
      },
      "outputs": [],
      "source": [
        "# creating dict that will contain word embeddings for words found in the glove_word_embeddings.txt\n",
        "word_embeddings_dict = {}\n",
        "glove_word_embeddings = open('glove_word_embeddings.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_word_embeddings:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embedding_vector = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings_dict[word] = embedding_vector\n",
        "glove_word_embeddings.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LAqLuvq9cwzJ"
      },
      "outputs": [],
      "source": [
        "# each row corresponds to the index of the word in the corpus\n",
        "# the matrix has 100 columns, where each column contains the glove embeddings for the words in the corpus\n",
        "# matrix can now be used as an initial embedding layer when training the neural network\n",
        "embedding_matrix = np.zeros((corpus_length, 100))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = word_embeddings_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkm4ut_cfxWt",
        "outputId": "6e8502ed-4d39-4a9a-988c-598864557433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(90095, 100)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_matrix.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
